"""prompted.specifications.openai.types

Contains various types that define the specification for the
`openai` API provider for language models.
"""

from pydantic import BaseModel, RootModel
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import TypeAliasType

__all__ = [
    "OpenAIMessageRole",
    "OpenAIFunctionParameters",
    "OpenAIFunction",
    "OpenAITool",
    "OpenAIImageUrl",
    "OpenAIInputAudio",
    "OpenAIAudioUrl",
    "OpenAIToolCallFunction",
    "OpenAIToolCall",
    "OpenAIMessageContentImagePart",
    "OpenAIMessageContentAudioPart",
    "OpenAIMessageContentTextPart",
    "OpenAIMessageContentRefusalPart",
    "OpenAIMessageContentPart",
    "OpenAISystemMessage",
    "OpenAIUserMessage",
    "OpenAIToolMessage",
    "OpenAIAssistantMessage",
    "OpenAIMessage",
    "OpenAITopLogprob",
    "OpenAITokenLogprob",
    "OpenAIResponseChoiceLogprobs",
    "OpenAIResponseUsage",
    "OpenAIResponseChoice",
    "OpenAIResponse",
    "OpenAIStreamToolCall",
    "OpenAIStreamChoiceDelta",
    "OpenAIStreamChoice",
    "OpenAIStreamChunk",
    "OpenAIEmbeddingUsage",
    "OpenAIEmbedding",
]


# ------------------------------------------------------------
# Generic Aliases & Type Definitions (Non-Models)
# ------------------------------------------------------------


OpenAIMessageRole = TypeAliasType(
    "OpenAIMessageRole", Literal["user", "assistant", "tool", "system", "developer"]
)
"""
Type definition for accepted message roles within a message in the OpenAI chat completions
specification.
"""


OpenAIFunctionParameters = TypeAliasType("OpenAIFunctionParameters", Dict[str, object])
"""Type alias for the parameters within the definition for a tool/function."""


# ------------------------------------------------------------
# Tools
# ------------------------------------------------------------


class OpenAIFunction(BaseModel):
    """
    A function that is defined within a tool's definition in the
    OpenAI format.
    """

    name: str
    """
    The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
    underscores and dashes, with a maximum length of 64.
    """
    parameters: OpenAIFunctionParameters
    """
    The parameters the functions accepts, described as a JSON Schema object.
    See the OpenAI guide for examples: https://platform.openai.com/docs/guides/function-calling
    and the JSON Schema reference for documentation: https://json-schema.org/understanding-json-schema/
    """
    description: Optional[str] = None
    """
    A description of what the function does, used by the model to choose when and
    how to call the function.
    """
    strict: Optional[bool] = None
    """
    Whether to enable strict schema adherence when generating the function call.
    If set to true, the model will follow the exact schema defined in the parameters field.
    Only a subset of JSON Schema is supported when strict is true.
    """


class OpenAITool(BaseModel):
    """
    A tool that is defined within a tool's definition in the
    OpenAI format.
    """

    type: Literal["function"]
    """
    The type of the tool. Currently, only `function` is supported.
    """
    function: OpenAIFunction
    """
    The function that the tool calls."""


class OpenAIImageUrl(BaseModel):
    """
    Represents the url to an image defined within a message content
    part.
    """

    url: str
    """The URL of the image."""
    detail: Optional[Literal["auto", "low", "high"]] = None
    """
    The detail level of this image.

    - `auto` lets the model decide.
    - `low` uses fewer tokens.
    - `high` uses more tokens.

    Defaults to `auto`.
    """


class OpenAIInputAudio(BaseModel):
    """
    Definition for an audio input usable within chat completion
    message content parts.
    """

    data: str
    """The base64 encoded audio data."""
    format: Optional[Literal["wav", "mp3", "opus", "flac", "aac", "m4a"]] = None
    """The format of the audio data."""


class OpenAIAudioUrl(BaseModel):
    """
    Definition for an audio input usable within chat completion
    message content parts.
    """

    url: str
    """The URL of the audio data."""


class OpenAIToolCallFunction(BaseModel):
    """
    Represents the 'function' key within a tool call in a completion
    response / assistant message.
    """

    arguments: str
    """
    The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
    hallucinate parameters not defined by your function schema. Validate the
    arguments in your code before calling your function.
    """
    name: str
    """The name of the function to call."""


class OpenAIToolCall(BaseModel):
    """
    A tool call within a completion response / assistant message in the
    OpenAI chat completions specification.
    """

    type: Literal["function"]
    """The type of the tool call.
    
    Currently, only `function` is supported."""
    id: str
    """The ID of the tool call."""
    function: OpenAIToolCallFunction
    """The function to call."""


# ------------------------------------------------------------------------------
# Message Content Parts
# ------------------------------------------------------------------------------


class OpenAIMessageContentImagePart(BaseModel):
    """
    A part within a chat completion message that represents/contains image
    content.
    """

    type: Literal["image_url"]
    """The type of this content part."""
    image_url: OpenAIImageUrl
    """
    The image to be sent / displayed. Represented as a
    URL.
    """


class OpenAIMessageContentAudioPart(BaseModel):
    """
    A part within a chat completion message that represents/contains audio
    content.

    NOTE: Only one of `audio_url` or `input_audio` can be used.
    """

    type: Literal["audio_url", "input_audio"]
    """The type of this content part.
    
    Can be either `audio_url` or `input_audio`."""
    audio_url: Optional[OpenAIAudioUrl] = None
    """
    The URL of the audio data.
    """
    input_audio: Optional[OpenAIInputAudio] = None
    """
    The base64 encoded audio data.
    """


class OpenAIMessageContentTextPart(BaseModel):
    """
    A part within a chat completion message that represents/contains text
    content.
    """

    type: Literal["text"]
    """The type of this content part."""
    text: str
    """The text of the message."""


class OpenAIMessageContentRefusalPart(BaseModel):
    """
    A part within a chat completion message that represents/contains refusal
    content.

    NOTE: This is a special case, only assistant messages can return
    or use refusal content parts.
    """

    type: Literal["refusal"]
    """The type of this content part."""
    refusal: str
    """The refusal message by the assistant."""


class OpenAIMessageContentPart(
    RootModel[
        OpenAIMessageContentImagePart
        | OpenAIMessageContentAudioPart
        | OpenAIMessageContentTextPart
        | OpenAIMessageContentRefusalPart
    ]
):
    """
    A part within a chat completion message that represents/contains content.
    """

    root: Union[
        OpenAIMessageContentImagePart,
        OpenAIMessageContentAudioPart,
        OpenAIMessageContentTextPart,
        OpenAIMessageContentRefusalPart,
    ]
    """The root of the message content part."""


# ------------------------------------------------------------------------------
# Message Types
# ------------------------------------------------------------------------------


class OpenAISystemMessage(BaseModel):
    """
    A system message in the OpenAI chat completions specification.
    """

    model_config = {"extra": "allow"}

    role: Literal["system"] = "system"
    """The role of this message. (Always `system`)"""
    content: Union[str, List[OpenAIMessageContentPart]]
    """
    The content of this message.
    """
    name: Optional[str] = None
    """An optional name for the participant."""


class OpenAIUserMessage(BaseModel):
    """
    A user message in the OpenAI chat completions specification.
    """

    model_config = {"extra": "allow"}

    role: Literal["user"] = "user"
    """The role of this message. (Always `user`)"""
    content: Union[str, List[OpenAIMessageContentPart]]
    """
    The content of this message.
    NOTE: User messages can not have 'None' message content.
    """
    name: Optional[str] = None
    """An optional name for the participant."""


class OpenAIToolMessage(BaseModel):
    """
    A tool message in the OpenAI chat completions specification.
    """

    model_config = {"extra": "allow"}

    role: Literal["tool"] = "tool"
    """The role of this message. (Always `tool`)"""
    content: Union[str, List[OpenAIMessageContentTextPart]]
    """The content of this message.
    Can be a string or a list of message content **TEXT** parts."""
    tool_call_id: str
    """The ID of the tool call that this message is responding to."""
    name: Optional[str] = None
    """
    Provides the model information to differentiate between participants of the same
    role.
    """


class OpenAIAssistantMessage(BaseModel):
    """
    An assistant message in the OpenAI chat completions specification.
    """

    model_config = {"extra": "allow"}

    role: Literal["assistant"]
    """The role of this message. (Always `assistant`)"""
    content: Union[
        str, List[Union[OpenAIMessageContentTextPart, OpenAIMessageContentRefusalPart]]
    ]
    """The content of this message.
    
    Can be a string or a list of message content parts.
    """
    refusal: Optional[str] = None
    """The refusal message by the assistant."""

    function_call: Optional[Any] = None
    """Function call response.
    NOTE:
    This is deprecated in favor of `tool_calls`.
    """
    tool_calls: Optional[List[OpenAIToolCall]] = None
    """Tool calls created by the assistant."""
    name: Optional[str] = None
    """
    Provides the model information to differentiate between participants of the same
    role.
    """


class OpenAIMessage(
    RootModel[
        OpenAISystemMessage
        | OpenAIUserMessage
        | OpenAIToolMessage
        | OpenAIAssistantMessage
    ]
):
    """
    A message in the OpenAI chat completions specification.
    """

    root: Union[
        OpenAISystemMessage, OpenAIUserMessage, OpenAIToolMessage, OpenAIAssistantMessage
    ]
    """The root of the message."""


class OpenAITopLogprob(BaseModel):
    """
    Represents the log probabilities of a token chosen as the top choice.
    """
    token: str
    """
    The token.
    """
    bytes: Optional[List[int]] = None
    """
    A list of integers representing the UTF-8 bytes representation of the token.
    Useful in instances where characters are represented by multiple tokens.
    """
    logprob: float
    """
    The log probability of this token.
    """


class OpenAITokenLogprob(BaseModel):
    """
    Represents the logprobs of a specific token, including top alternatives.
    """
    token: str
    """
    The token.
    """
    bytes: Optional[List[int]] = None
    """
    A list of integers representing the UTF-8 bytes representation of the token.
    Useful in instances where characters are represented by multiple tokens.
    """
    logprob: float
    """
    The log probability of this token.
    """
    top_logprobs: List[OpenAITopLogprob]
    """
    List of the most likely tokens and their logprobs, at this token position.
    In rare cases, there may be fewer than `top_logprobs` returned.
    """


class OpenAIResponseChoiceLogprobs(BaseModel):
    """
    Log probability information for the choice.
    """
    content: Optional[List[OpenAITokenLogprob]] = None
    """
    A list of message content tokens with log probability information.
    """


class OpenAIResponseUsage(BaseModel):
    """
    Represents the usage information for a completion in the
    OpenAI specification.
    """
    completion_tokens: int
    """Number of tokens in the generated completion."""
    prompt_tokens: int
    """Number of tokens in the prompt."""
    total_tokens: int
    """Total number of tokens used in the request (prompt + completion)."""


class OpenAIResponseChoice(BaseModel):
    """
    A choice within a non-streamed OpenAI chat completion response.
    """
    message : OpenAIAssistantMessage
    """The message that was generated by the model."""
    finish_reason: Literal[
        "stop",
        "length",
        "tool_calls",
        "content_filter",
        "function_call",
    ]  # "function_call" is deprecated
    """
    The reason the model stopped generating tokens.
    - `stop`: API returned complete message, or a message terminated by a stop sequence.
    - `length`: Incomplete model output due to `max_tokens` parameter or token limit.
    - `tool_calls`: Model called a tool.
    - `content_filter`: Omitted content due to a flag from our content filters.
    - `function_call`: (Deprecated) Model called a function.
    """
    index: int
    """
    The index of this choice in the list of choices.
    """
    logprobs: Optional[OpenAIResponseChoiceLogprobs] = None
    """
    Log probability information for the choice.
    """


class OpenAIResponse(BaseModel):
    """
    A response from an LLM in the specification of the OpenAI
    Chat Completions API.
    """
    id: str
    """
    A unique identifier for the chat completion.
    """
    choices: List[OpenAIResponseChoice]
    """
    A list of chat completion choices. Can be more than one if `n` is greater
    than 1.
    """
    created: int
    """
    The Unix timestamp (in seconds) of when the chat completion was created.
    """
    model: str
    """
    The model used for the chat completion.
    """
    object: Literal["chat.completion"]  # OpenAI SDK uses 'chat.completion'
    """
    The object type, which is always `chat.completion`.
    """
    system_fingerprint: Optional[str] = None
    """
    This fingerprint represents the backend configuration that the model runs with.
    You can use this value to track changes in the backend before comparing output
    from different API calls.
    """
    usage: Optional[OpenAIResponseUsage] = None
    """
    Usage statistics for the completion request.
    """


# ------------------------------------------------------------------------------
# STREAMING
# ------------------------------------------------------------------------------


class OpenAIStreamToolCall(OpenAIToolCall):
    """
    Format for tool calls in streamed chunks. This model extends
    standard tool calls by providing an 'index' field, which allows
    for identifying the tool call in the list of tool calls.
    """
    index: Optional[int] = None
    """
    The index of the tool call in the list of tool calls.
    """


class OpenAIStreamChoiceDelta(BaseModel):
    """
    Represents the delta message content for a streaming choice.
    """
    role: Optional[OpenAIMessageRole] = None
    """
    The role of the author of this message.
    """
    content : Optional[str] = None
    """The contents of the chunk message."""
    tool_calls: Optional[List[OpenAIStreamToolCall]] = None
    """
    Tool calls generated by the model. Can appear incrementally.
    Each tool call in the list can have an `index` field.
    """
    function_call: Optional[OpenAIToolCallFunction] = None
    """
    (Deprecated) The name and arguments of a function that should be called.
    """


class OpenAIStreamChoice(BaseModel):
    """
    Represents a choice within streamed chunks in the OpenAI chat completions
    format.
    """
    delta: OpenAIStreamChoiceDelta
    """
    A chat completion delta generated by streamed model responses.
    """
    finish_reason: Optional[
        Literal[
            "stop",
            "length",
            "tool_calls",
            "content_filter",
            "function_call",
        ]
    ] = None  # "function_call" is deprecated
    """
    The reason the model stopped generating tokens.
    Present in the final chunk of a choice.
    """
    index: int
    """
    The index of this choice in the stream.
    """
    logprobs: Optional[OpenAIResponseChoiceLogprobs] = None  
    """
    Log probability information for the choice.
    """


class OpenAIStreamChunk(BaseModel):
    """
    A single chunk within a streamed response from an OpenAI
    chat completion.
    """
    id: str
    """
    A unique identifier for the chat completion chunk.
    """
    choices: List[OpenAIStreamChoice]
    """
    A list of chat completion choices. Can be more than one if `n` is greater
    than 1.
    """
    created: int
    """
    The Unix timestamp (in seconds) of when the chat completion chunk was created.
    """
    model: str
    """
    The model to generate the completion.
    """
    object: Literal["chat.completion.chunk"]  # OpenAI SDK uses 'chat.completion.chunk'
    """
    The object type, which is always `chat.completion.chunk`.
    """
    system_fingerprint: Optional[str] = None
    """
    This fingerprint represents the backend configuration that the model runs with.
    You can use this value to track changes in the backend before comparing output
    from different API calls.
    """
    usage: Optional[OpenAIResponseUsage] = (
        None  # Usage is often null in chunks until the very end, or not present at all.
    )
    """
    An optional field that appears when the model is `gpt-3.5-turbo-0125` or `gpt-4-turbo-preview`
    and the `stream_options` parameter is set.
    It contains token usage statistics for the entire request, summed across all chunks.
    """


# ------------------------------------------------------------------------------
# EMBEDDINGS
# ------------------------------------------------------------------------------


class OpenAIEmbeddingUsage(BaseModel):
    """
    Represents the usage information for an embedding request in the
    OpenAI specification.
    """
    prompt_tokens: int
    """Number of tokens in the input prompt."""
    total_tokens: int
    """Total number of tokens used in the request (prompt + embedding)."""


class OpenAIEmbedding(BaseModel):
    """
    Represents an embedding vector returned by the embedding endpoint.
    """
    embedding: List[float]
    """
    The embedding vector, which is a list of floats. The length of vector depends
    on the model.
    """
    index: int
    """
    The index of the embedding in the list of embeddings.
    """
    object: Literal["embedding"]
    """
    The object type, which is always `embedding`.
    """