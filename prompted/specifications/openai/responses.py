"""
prompted.specifications.openai.responses

Contains type definitions for outputs in the
OpenAI chat completions specification (referred to as
'completions' by OpenAI).
"""

from typing import List, Literal, Optional
from pydantic import BaseModel

from .messages import (
    OpenAIToolCall,
    OpenAIToolCallFunction,
    OpenAIMessageRole,
    OpenAIAssistantMessage
)

__all__ = [
    "OpenAITopLogprob",
    "OpenAITokenLogprob",
    "OpenAIResponseChoiceLogprobs",
    "OpenAIResponseUsage",
    "OpenAIResponseChoice",
    "OpenAIResponse",
    "OpenAIStreamToolCall",
    "OpenAIStreamChoiceDelta",
    "OpenAIStreamChoice",
    "OpenAIStreamChunk",
]


# ------------------------------------------------------------------------------
# Generic Types
# ------------------------------------------------------------------------------


class OpenAITopLogprob(BaseModel):
    """
    Represents the log probabilities of a token chosen as the top choice.
    """
    token: str
    """
    The token.
    """
    bytes: Optional[List[int]] = None
    """
    A list of integers representing the UTF-8 bytes representation of the token.
    Useful in instances where characters are represented by multiple tokens.
    """
    logprob: float
    """
    The log probability of this token.
    """


class OpenAITokenLogprob(BaseModel):
    """
    Represents the logprobs of a specific token, including top alternatives.
    """
    token: str
    """
    The token.
    """
    bytes: Optional[List[int]] = None
    """
    A list of integers representing the UTF-8 bytes representation of the token.
    Useful in instances where characters are represented by multiple tokens.
    """
    logprob: float
    """
    The log probability of this token.
    """
    top_logprobs: List[OpenAITopLogprob]
    """
    List of the most likely tokens and their logprobs, at this token position.
    In rare cases, there may be fewer than `top_logprobs` returned.
    """


class OpenAIResponseChoiceLogprobs(BaseModel):
    """
    Log probability information for the choice.
    """
    content: Optional[List[OpenAITokenLogprob]] = None
    """
    A list of message content tokens with log probability information.
    """


class OpenAIResponseUsage(BaseModel):
    """
    Represents the usage information for a completion in the
    OpenAI specification.
    """
    completion_tokens: int
    """Number of tokens in the generated completion."""
    prompt_tokens: int
    """Number of tokens in the prompt."""
    total_tokens: int
    """Total number of tokens used in the request (prompt + completion)."""


class OpenAIResponseChoice(BaseModel):
    """
    A choice within a non-streamed OpenAI chat completion response.
    """
    message : OpenAIAssistantMessage
    """The message that was generated by the model."""
    finish_reason: Literal[
        "stop",
        "length",
        "tool_calls",
        "content_filter",
        "function_call",
    ]  # "function_call" is deprecated
    """
    The reason the model stopped generating tokens.
    - `stop`: API returned complete message, or a message terminated by a stop sequence.
    - `length`: Incomplete model output due to `max_tokens` parameter or token limit.
    - `tool_calls`: Model called a tool.
    - `content_filter`: Omitted content due to a flag from our content filters.
    - `function_call`: (Deprecated) Model called a function.
    """
    index: int
    """
    The index of this choice in the list of choices.
    """
    logprobs: Optional[OpenAIResponseChoiceLogprobs] = None
    """
    Log probability information for the choice.
    """


class OpenAIResponse(BaseModel):
    """
    A response from an LLM in the specification of the OpenAI
    Chat Completions API.
    """
    id: str
    """
    A unique identifier for the chat completion.
    """
    choices: List[OpenAIResponseChoice]
    """
    A list of chat completion choices. Can be more than one if `n` is greater
    than 1.
    """
    created: int
    """
    The Unix timestamp (in seconds) of when the chat completion was created.
    """
    model: str
    """
    The model used for the chat completion.
    """
    object: Literal["chat.completion"]  # OpenAI SDK uses 'chat.completion'
    """
    The object type, which is always `chat.completion`.
    """
    system_fingerprint: Optional[str] = None
    """
    This fingerprint represents the backend configuration that the model runs with.
    You can use this value to track changes in the backend before comparing output
    from different API calls.
    """
    usage: Optional[OpenAIResponseUsage] = None
    """
    Usage statistics for the completion request.
    """


# ------------------------------------------------------------------------------
# STREAMING
# ------------------------------------------------------------------------------


class OpenAIStreamToolCall(OpenAIToolCall):
    """
    Format for tool calls in streamed chunks. This model extends
    standard tool calls by providing an 'index' field, which allows
    for identifying the tool call in the list of tool calls.
    """
    index: Optional[int] = None
    """
    The index of the tool call in the list of tool calls.
    """


class OpenAIStreamChoiceDelta(BaseModel):
    """
    Represents the delta message content for a streaming choice.
    """
    role: Optional[OpenAIMessageRole] = None
    """
    The role of the author of this message.
    """
    content : Optional[str] = None
    """The contents of the chunk message."""
    tool_calls: Optional[List[OpenAIStreamToolCall]] = None
    """
    Tool calls generated by the model. Can appear incrementally.
    Each tool call in the list can have an `index` field.
    """
    function_call: Optional[OpenAIToolCallFunction] = None
    """
    (Deprecated) The name and arguments of a function that should be called.
    """


class OpenAIStreamChoice(BaseModel):
    """
    Represents a choice within streamed chunks in the OpenAI chat completions
    format.
    """
    delta: OpenAIStreamChoiceDelta
    """
    A chat completion delta generated by streamed model responses.
    """
    finish_reason: Optional[
        Literal[
            "stop",
            "length",
            "tool_calls",
            "content_filter",
            "function_call",
        ]
    ] = None  # "function_call" is deprecated
    """
    The reason the model stopped generating tokens.
    Present in the final chunk of a choice.
    """
    index: int
    """
    The index of this choice in the stream.
    """
    logprobs: Optional[OpenAIResponseChoiceLogprobs] = None  
    """
    Log probability information for the choice.
    """


class OpenAIStreamChunk(BaseModel):
    """
    A single chunk within a streamed response from an OpenAI
    chat completion.
    """
    id: str
    """
    A unique identifier for the chat completion chunk.
    """
    choices: List[OpenAIStreamChoice]
    """
    A list of chat completion choices. Can be more than one if `n` is greater
    than 1.
    """
    created: int
    """
    The Unix timestamp (in seconds) of when the chat completion chunk was created.
    """
    model: str
    """
    The model to generate the completion.
    """
    object: Literal["chat.completion.chunk"]  # OpenAI SDK uses 'chat.completion.chunk'
    """
    The object type, which is always `chat.completion.chunk`.
    """
    system_fingerprint: Optional[str] = None
    """
    This fingerprint represents the backend configuration that the model runs with.
    You can use this value to track changes in the backend before comparing output
    from different API calls.
    """
    usage: Optional[OpenAIResponseUsage] = (
        None  # Usage is often null in chunks until the very end, or not present at all.
    )
    """
    An optional field that appears when the model is `gpt-3.5-turbo-0125` or `gpt-4-turbo-preview`
    and the `stream_options` parameter is set.
    It contains token usage statistics for the entire request, summed across all chunks.
    """