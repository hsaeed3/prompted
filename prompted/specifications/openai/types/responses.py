"""
prompted.specifications.openai.types.responses

Contains type definitions for outputs in the
OpenAI chat completions specification (referred to as
'completions' by OpenAI).
"""

from typing import List, Literal
from typing_extensions import Required, TypedDict, NotRequired

from .messages import (
    OpenAIMessageRole,
    OpenAIAssistantMessage,
)
from .tool_calls import OpenAIToolCall, OpenAIToolCallFunction

__all__ = [
    "OpenAITopLogprob",
    "OpenAITokenLogprob",
    "OpenAIResponseChoiceLogprobs",
    "OpenAIResponseUsage",
    "OpenAIResponseChoice",
    "OpenAIResponse",
    "OpenAIStreamToolCall",
    "OpenAIStreamChoiceDelta",
    "OpenAIStreamChoice",
    "OpenAIStreamChunk",
]


# ------------------------------------------------------------------------------
# Generic Types
# ------------------------------------------------------------------------------


class OpenAITopLogprob(TypedDict):
    """
    Represents the log probabilities of a token chosen as the top choice.
    """

    token: Required[str]
    """
    The token.
    """
    bytes: NotRequired[List[int]]
    """
    A list of integers representing the UTF-8 bytes representation of the token.
    Useful in instances where characters are represented by multiple tokens.
    """
    logprob: Required[float]
    """
    The log probability of this token.
    """


class OpenAITokenLogprob(TypedDict):
    """
    Represents the logprobs of a specific token, including top alternatives.
    """

    token: Required[str]
    """
    The token.
    """
    bytes: NotRequired[List[int]]
    """
    A list of integers representing the UTF-8 bytes representation of the token.
    Useful in instances where characters are represented by multiple tokens.
    """
    logprob: Required[float]
    """
    The log probability of this token.
    """
    top_logprobs: Required[List[OpenAITopLogprob]]
    """
    List of the most likely tokens and their logprobs, at this token position.
    In rare cases, there may be fewer than `top_logprobs` returned.
    """


class OpenAIResponseChoiceLogprobs(TypedDict):
    """
    Log probability information for the choice.
    """

    content: NotRequired[List[OpenAITokenLogprob]]
    """
    A list of message content tokens with log probability information.
    """


class OpenAIResponseUsage(TypedDict):
    """
    Represents the usage information for a completion in the
    OpenAI specification.
    """

    completion_tokens: Required[int]
    """Number of tokens in the generated completion."""
    prompt_tokens: Required[int]
    """Number of tokens in the prompt."""
    total_tokens: Required[int]
    """Total number of tokens used in the request (prompt + completion)."""


class OpenAIResponseChoice(TypedDict):
    """
    A choice within a non-streamed OpenAI chat completion response.
    """

    message: Required[
        OpenAIAssistantMessage
    ]  # Assumes OpenAIAssistantMessage is a TypedDict
    """The message that was generated by the model."""
    finish_reason: Required[
        Literal[
            "stop",
            "length",
            "tool_calls",
            "content_filter",
            "function_call",
        ]
    ]  # "function_call" is deprecated
    """
    The reason the model stopped generating tokens.
    - `stop`: API returned complete message, or a message terminated by a stop sequence.
    - `length`: Incomplete model output due to `max_tokens` parameter or token limit.
    - `tool_calls`: Model called a tool.
    - `content_filter`: Omitted content due to a flag from our content filters.
    - `function_call`: (Deprecated) Model called a function.
    """
    index: Required[int]
    """
    The index of this choice in the list of choices.
    """
    logprobs: NotRequired[OpenAIResponseChoiceLogprobs]
    """
    Log probability information for the choice.
    """


class OpenAIResponse(TypedDict):
    """
    A response from an LLM in the specification of the OpenAI
    Chat Completions API.
    """

    id: Required[str]
    """
    A unique identifier for the chat completion.
    """
    choices: Required[List[OpenAIResponseChoice]]
    """
    A list of chat completion choices. Can be more than one if `n` is greater
    than 1.
    """
    created: Required[int]
    """
    The Unix timestamp (in seconds) of when the chat completion was created.
    """
    model: Required[str]
    """
    The model used for the chat completion.
    """
    object: Required[Literal["chat.completion"]]
    """
    The object type, which is always `chat.completion`.
    """
    system_fingerprint: NotRequired[str]
    """
    This fingerprint represents the backend configuration that the model runs with.
    You can use this value to track changes in the backend before comparing output
    from different API calls.
    """
    usage: NotRequired[OpenAIResponseUsage]
    """
    Usage statistics for the completion request.
    """


# ------------------------------------------------------------------------------
# STREAMING
# ------------------------------------------------------------------------------


class OpenAIStreamToolCall(
    OpenAIToolCall
):  # Inherits from the (now) TypedDict OpenAIToolCall
    """
    Format for tool calls in streamed chunks. This model extends
    standard tool calls by providing an 'index' field, which allows
    for identifying the tool call in the list of tool calls.
    """

    index: NotRequired[int]
    """
    The index of the tool call in the list of tool calls.
    """


class OpenAIStreamChoiceDelta(TypedDict):
    """
    Represents the delta message content for a streaming choice.
    """

    role: NotRequired[OpenAIMessageRole]  # Assumes OpenAIMessageRole is a TypeAliasType
    """
    The role of the author of this message.
    """
    content: NotRequired[str]
    """The contents of the chunk message."""
    tool_calls: NotRequired[List[OpenAIStreamToolCall]]
    """
    Tool calls generated by the model. Can appear incrementally.
    Each tool call in the list can have an `index` field.
    """
    function_call: NotRequired[
        OpenAIToolCallFunction
    ]  # Assumes OpenAIToolCallFunction is a TypedDict
    """
    (Deprecated) The name and arguments of a function that should be called.
    """


class OpenAIStreamChoice(TypedDict):
    """
    Represents a choice within streamed chunks in the OpenAI chat completions
    format.
    """

    delta: Required[OpenAIStreamChoiceDelta]
    """
    A chat completion delta generated by streamed model responses.
    """
    finish_reason: NotRequired[
        Literal[
            "stop",
            "length",
            "tool_calls",
            "content_filter",
            "function_call",
        ]
    ]  # "function_call" is deprecated
    """
    The reason the model stopped generating tokens.
    Present in the final chunk of a choice.
    """
    index: Required[int]
    """
    The index of this choice in the stream.
    """
    logprobs: NotRequired[OpenAIResponseChoiceLogprobs]
    """
    Log probability information for the choice.
    """


class OpenAIStreamChunk(TypedDict):
    """
    A single chunk within a streamed response from an OpenAI
    chat completion.
    """

    id: Required[str]
    """
    A unique identifier for the chat completion chunk.
    """
    choices: Required[List[OpenAIStreamChoice]]
    """
    A list of chat completion choices. Can be more than one if `n` is greater
    than 1.
    """
    created: Required[int]
    """
    The Unix timestamp (in seconds) of when the chat completion chunk was created.
    """
    model: Required[str]
    """
    The model to generate the completion.
    """
    object: Required[Literal["chat.completion.chunk"]]
    """
    The object type, which is always `chat.completion.chunk`.
    """
    system_fingerprint: NotRequired[str]
    """
    This fingerprint represents the backend configuration that the model runs with.
    You can use this value to track changes in the backend before comparing output
    from different API calls.
    """
    usage: NotRequired[OpenAIResponseUsage]
    """
    An optional field that appears when the model is `gpt-3.5-turbo-0125` or `gpt-4-turbo-preview`
    and the `stream_options` parameter is set.
    It contains token usage statistics for the entire request, summed across all chunks.
    """
